{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_NeuralNetworks_WE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM55xYk_uKPv"
      },
      "source": [
        "\n",
        "## Word Embeding\n",
        "### ðŸš¨ 1Âº - Dar Upload no colab \n",
        "- Configuration of GPU mode:\n",
        "> Primeiro ir Ã  **tab** Runtime -> Change runtime type -> Hardware accelarator = GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7sTgssiyw7D"
      },
      "source": [
        "\n",
        "> Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQwqiHd2fOyh"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsULoFgeqNes"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "> Nota: Colocar tudo na drive como zip, pois Ã© mais rÃ¡pido !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vrwx3F1EH2a"
      },
      "source": [
        "#Colocar os dados na drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#!unzip \"/content/drive/My Drive/LEI/comments_processed.json.zip\" \n",
        "!unzip \"/content/drive/My Drive/LEI/glove.6B.zip\" \n",
        "!unzip \"/content/drive/My Drive/LEI/featherized.ftr.zip\"\n",
        "!unzip \"/content/drive/My Drive/LEI/beast_model_m.keras.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNA5kj6ra1s3"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "#load data into arrays\n",
        "#\n",
        "def load_Data(filename):\n",
        "  d_polarity = {'negative':0,'neutro':1, 'positive':2}\n",
        "  samples_0=[]\n",
        "  samples_1=[]\n",
        "  samples_2=[]\n",
        "  aux=False\n",
        "  if filename == 'featherized.ftr':\n",
        "    data = pd.read_feather('featherized.ftr', columns=None, use_threads=True)\n",
        "    aux=True\n",
        "  else:\n",
        "    f = open('comments_processed.json','r')\n",
        "    data=json.load(f)\n",
        "\n",
        "  #separate data by polarity\n",
        "  if aux:\n",
        "    for i,row in data.iterrows():\n",
        "      if row['polarityClass'] == 0:\n",
        "        samples_0.append(row['comentario']) #CHAMAR A FUNÃ‡Ã‚Ã‚O DO PREPROCESSAMENTO \n",
        "      elif row['polarityClass']  == 1:\n",
        "        samples_1.append(row['comentario'])\n",
        "      elif row['polarityClass']  == 2:\n",
        "        samples_2.append(row['comentario'])\n",
        "        \n",
        "  else:\n",
        "    for i in data:\n",
        "      if d_polarity[data[i]['rev_polarity']] == 0:\n",
        "        samples_0.append(data[i]['rev_text']) #CHAMAR A FUNÃ‡Ã‚Ã‚O DO PREPROCESSAMENTO \n",
        "      elif d_polarity[data[i]['rev_polarity']] == 1:\n",
        "        samples_1.append(data[i]['rev_text']) \n",
        "      elif d_polarity[data[i]['rev_polarity']] == 2:\n",
        "        samples_2.append(data[i]['rev_text'])\n",
        "\n",
        "  \n",
        "  len_0=len(samples_0)\n",
        "  len_1=len(samples_1)\n",
        "  len_2=len(samples_2)\n",
        "  \n",
        "  #create 3 datasets for train, validation and test (0.6 Train, 0.2 Validation, 0.2 Test)\n",
        "  \n",
        "  train_samples = samples_0[:int(0.6*len_0)]           + samples_1[:int(0.6*len_1)]          + samples_2[:int(0.6*len_2)]\n",
        "  train_labels= [0]* int(0.6*len_0)                    + [1]*int(0.6*len_1)                   + [2] * int(0.6*len_2)\n",
        "  \n",
        "  val_samples  =  samples_0[int(0.6*len_0):int(0.8*len_0)]  + samples_1[int(0.6*len_1):int(0.8*len_1)] + samples_2[int(0.6*len_2):int(0.8*len_2)] \n",
        "  val_labels = [0]*(int(0.8*len_0) - int(0.6*len_0))        + [1]*(int(0.8*len_1) - int(0.6*len_1))    + [2] * (int(0.8*len_2) - int(0.6*len_2)) \n",
        "  \n",
        "  test_samples =  samples_0[int(0.8*len_0):]           +  samples_1[int(0.8*len_1):]         + samples_2[int(0.8*len_2):]\n",
        "  test_labels = [0] * (len_0 - int(0.8*len_0))         + [1] * (len_1 - int(0.8*len_1))       + [2] * (len_2 - int(0.8*len_2))       \n",
        "  \n",
        "  \n",
        "  #shuffle data\n",
        "  \n",
        "  import random\n",
        "  c = list(zip(train_samples, train_labels))\n",
        "  random.shuffle(c)\n",
        "  train_samples, train_labels = zip(*c)\n",
        "  \n",
        "  c = list(zip(val_samples, val_labels))\n",
        "  random.shuffle(c)\n",
        "  val_samples, val_labels = zip(*c)\n",
        "  \n",
        "  c = list(zip(test_samples, test_labels))\n",
        "  random.shuffle(c)\n",
        "  test_samples, test_labels = zip(*c)\n",
        "  return train_samples, train_labels, val_samples, val_labels,test_samples, test_labels\n",
        "train_samples, train_labels, val_samples, val_labels,test_samples, test_labels=load_Data('featherized.ftr')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gos0z012O_dI"
      },
      "source": [
        "'''\n",
        "#verificaÃ§Ã£Ã£o dos dados.\n",
        "print(train_samples[0])\n",
        "print(train_labels[0])\n",
        "for s in samples_0:\n",
        "  if s==train_samples[0]:\n",
        "    print(\"YES_0\")\n",
        "    print(\"\\t \"+ s )\n",
        "      \n",
        "for s in samples_1:\n",
        "  if s==train_samples[0]:\n",
        "    print(\"YES_1\")\n",
        "    print(\"\\t \"+ s )\n",
        "\n",
        "for s in samples_2:\n",
        "  if s==train_samples[0]:\n",
        "    print(\"YES_2\")\n",
        "    print(\"\\t \"+ s )\n",
        "      \n",
        "'''   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMgQmmQS2_v"
      },
      "source": [
        "## Create a vocabulary index\n",
        "\n",
        "> Our layer will only consider the top 20,000 words, and will truncate or pad sequences to be actually 200 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfosM_aNhXJy"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import pickle\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices([str(x) for x in train_samples]).batch(128)#.batch(128)\n",
        "vectorizer.adapt(text_ds) # give context to the network\n",
        "\n",
        "#Dump vectorizer to a file \n",
        "#pickle.dump({'config': vectorizer.get_config(),'weights': vectorizer.get_weights()}, open(\"/content/drive/My Drive/tv_layer.pkl\", \"wb\"))\n",
        "\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2FLdBMbspiP"
      },
      "source": [
        "> Donwload Global Vectors for Word Representation from site.<br/>\n",
        "\n",
        ">Note: It can be stored in the drive for faster loading. The code is in the load data section above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipBQ701iEeB"
      },
      "source": [
        "#Donwload pre trained model\n",
        "#https://nlp.stanford.edu/projects/glove/\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcnWfJLJtOG3"
      },
      "source": [
        "> Load vector from file and create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AeQiHTuiKpd"
      },
      "source": [
        "path_to_glove_file =\"glove.6B.300d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYphMd_VjFm-"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhA4vM5iGkM"
      },
      "source": [
        "> Plot the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VSIhbQpd1WV"
      },
      "source": [
        "#plot words \n",
        "#https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "words =  list(embeddings_index.keys())\n",
        "vectors = [embeddings_index[word] for word in words]\n",
        "Y = tsne.fit_transform(vectors[:1000])\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "\n",
        "for label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJKJ5IzFjMlE"
      },
      "source": [
        "> Processing data for glove embeding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N8hIJC3jKKj"
      },
      "source": [
        "#prepare data for CNN\n",
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "#y_train = np.array(train_labels,dtype=np.float32)\n",
        "#y_val = np.array(val_labels,dtype=np.float32)\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9HkNFJ7j8Yg"
      },
      "source": [
        "#plot data\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "bar=sns.barplot(unique_elements,counts_elements, palette='mako')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkOVxWfrq_w-"
      },
      "source": [
        "## Balance Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXi6n7uymbCv"
      },
      "source": [
        "#!pip install imblearn\n",
        "#!pip install imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9bFMuFTsOGK"
      },
      "source": [
        "%%time\n",
        "\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "#Estrategias para fazer under e over sampling\n",
        "strategy = {0:7000,1:7000} # over na classe 0 e 1.\n",
        "strategy2 = {0:7000,1:7000,2:7000} #under nas 3 classes.\n",
        "#sm = SMOTE(\"minority\")#(random_state = 2,sampling_strategy=strategy)\n",
        "over = RandomOverSampler(sampling_strategy=strategy)\n",
        "#under = TomekLinks()\n",
        "under = RandomUnderSampler(sampling_strategy=strategy2)\n",
        "XX,YY=over.fit_sample(x_train,y_train)\n",
        "X,Y=under.fit_resample(XX,YY)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnl8iwD9uMnA"
      },
      "source": [
        "#print dataset\n",
        "unique_elements, counts_elements = np.unique(Y, return_counts=True)\n",
        "bar=sns.barplot(unique_elements,counts_elements, palette='mako')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-A6rlwkqh1R"
      },
      "source": [
        "> **CallBacks** for Models<br/>\n",
        "> Available callbacks on  https://keras.io/api/callbacks/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-a1MbFUjerh"
      },
      "source": [
        "\n",
        "filename=\"weights_best.h5\"\n",
        "cb2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "cb3 = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath= filename,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',#val_loss',#'val_mean_squared_error',\n",
        "    mode='min',\n",
        "    save_best_only= True)\n",
        "\n",
        "cb4 = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3) #se apartir de 2 epochs a loss nÃ£o descer,o learning rate diminui em 10%.\n",
        "my_callbacks=[\n",
        "    cb3#,cb2#,cb4\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Zv1TWgt4aX"
      },
      "source": [
        "##1Âº Model- CNN \n",
        "> The Functional API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_R_tAlLjO4H"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "embedding_layer = layers.Embedding(num_tokens,embedding_dim,\n",
        "                            embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n",
        "                            trainable=False,)\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(416, 1, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D()(x)\n",
        "x = layers.Conv1D(496, 1, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D()(x)\n",
        "x = layers.Conv1D(320, 1, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "preds = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "#model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV1FLIuvuiyj"
      },
      "source": [
        "> Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhW9yBPHjU6E"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='Adamax',\n",
        "    loss=\"sparse_categorical_crossentropy\", \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val),  callbacks=my_callbacks )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLamflw-TA2X"
      },
      "source": [
        "> For **see the results**, go to section \"Predict data and Visualization of results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO47dB8fjGfN"
      },
      "source": [
        "## 2Âº Model - RNN \n",
        "> LSTM layres\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB4p-1nMjEQg"
      },
      "source": [
        "embedding_layer = layers.Embedding(num_tokens,embedding_dim,\n",
        "                  embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n",
        "                  trainable=False,)\n",
        "\n",
        "model = keras.Sequential([\n",
        "      embedding_layer,\n",
        "      #keras.layers.Bidirectional(tf.keras.layers.LSTM(128,activation='tanh',return_sequences=True,dropout=0.1)),\n",
        "      layers.LSTM(128,activation='tanh',return_sequences=True,dropout=0.1),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(\n",
        "        units=64,\n",
        "        activation='relu'\n",
        "      ),\n",
        "      layers.Dropout(0.1),\n",
        "      layers.Dense(\n",
        "        units=64,\n",
        "        activation='relu'\n",
        "      ),\n",
        "      layers.Dense(3, activation= 'softmax')\n",
        "])\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9G3iKP0Gj4u"
      },
      "source": [
        "> Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y9LjgnZGLa1"
      },
      "source": [
        "model.compile(optimizer='Adamax',#hp.Choice('Optimizer', values = optimizer), #1e-2,\n",
        "              loss='sparse_categorical_crossentropy',#tf.keras.losses.CategoricalCrossentropy()\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val), callbacks=my_callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq5PxcJJGqIA"
      },
      "source": [
        "> For **see the results**, go to section \"Predict data and Visualization of results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iud8ggTB52cc"
      },
      "source": [
        "## Keras Tuner\n",
        "> Calculate the best parameters for the networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9RGVzZ52rs"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecDVay8PuGuQ"
      },
      "source": [
        "#genes\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evvF6l136CGE"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "def build_model(hp):  \n",
        "    embedding_layer = layers.Embedding(num_tokens,embedding_dim,\n",
        "                            embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n",
        "                            trainable=False,)\n",
        "\n",
        "#    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "#    embedded_sequences = embedding_layer(int_sequences_input)\n",
        "    ker_size = hp.Choice('kernel_Size', values = [1,3]),\n",
        "    model = keras.Sequential([\n",
        "      embedding_layer,\n",
        "      layers.Conv1D(hp.Int('Conv_1_filter', min_value=128, max_value=512, step=16),ker_size , activation=\"relu\"),\n",
        "      layers.MaxPooling1D(),\n",
        "      layers.Conv1D(hp.Int('Conv_2_filter', min_value=128, max_value=512, step=16), ker_size, activation=\"relu\"),\n",
        "      layers.MaxPooling1D(),\n",
        "      layers.Dropout(0.1),\n",
        "      layers.Conv1D(hp.Int('Conv_3_filter', min_value=128, max_value=512, step=16), ker_size, activation=\"relu\"),\n",
        "      #layers.MaxPooling1D(),\n",
        "      \n",
        "      layers.Dropout(0.2),\n",
        "      layers.LSTM(128,return_sequences=True,dropout=0.1),   \n",
        "      layers.GlobalMaxPooling1D(),\n",
        "\n",
        "      layers.Dense(\n",
        "         units=64,#hp.Int('Dense_1_units', min_value=128, max_value=1024, step=16),\n",
        "        activation='relu'\n",
        "      ),\n",
        "      layers.Dropout(0.1),\n",
        "      layers.Dense(\n",
        "        units=64,#hp.Int('Dense_1_units', min_value=128, max_value=1024, step=16),\n",
        "        activation='relu'\n",
        "      ),\n",
        "      layers.Dense(3, activation= 'softmax')#\n",
        "    ])\n",
        "  \n",
        "    model.compile(optimizer='Adamax',#hp.Choice('Optimizer', values = optimizer), #1e-2,\n",
        "              loss='sparse_categorical_crossentropy',#tf.keras.losses.CategoricalCrossentropy()\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "    return model\n",
        "  #optimizer='rmsprop'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkOpB5uC6FrG"
      },
      "source": [
        "from kerastuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "tuner_search=RandomSearch(build_model,\n",
        "                          #objective='val_accuracy',\n",
        "                          objective='val_loss',\n",
        "                          max_trials=20,directory='output',project_name=\"WE\")\n",
        "\n",
        "\n",
        "tuner_search.search(x_train,y_train,validation_data=(x_val, y_val),epochs=10,callbacks=my_callbacks)\n",
        "#Best parameters\n",
        "best_hyperparameters = tuner_search.get_best_hyperparameters(1)[0]\n",
        "print(best_hyperparameters.values)\n",
        "#get best model\n",
        "model=tuner_search.get_best_models(num_models=1)[0]\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u16KT-tPlFD9"
      },
      "source": [
        "> Find the best model in tunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al3SoeFUlEPK"
      },
      "source": [
        "#get best model\n",
        "model=tuner_search.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner_search.get_best_hyperparameters(1)[0]\n",
        "print(best_hyperparameters.values)\n",
        "model.summary()\n",
        "#model.compile(\n",
        "#    loss='sparse_categorical_crossentropy',#[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], \n",
        "#    optimizer=tf.keras.optimizers.Adagrad() , metrics=[\"accuracy\"]\n",
        "#)\n",
        "#model.fit(x_train, y_train, batch_size=64, epochs=15, validation_data=(x_val, y_val),   callbacks=my_callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvijIp_PLZYl"
      },
      "source": [
        "## Best Model - CRNN \n",
        "> CNN layers and RNN layers (MIXED)\n",
        "- Verificar a mÃ©trica **mcc metric**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NhvvrjroaQg"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "embedding_layer = layers.Embedding(num_tokens,embedding_dim,\n",
        "                  embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n",
        "                  trainable=False,)\n",
        "\n",
        "model = keras.Sequential([\n",
        "      embedding_layer,\n",
        "      layers.Conv1D(416, 1, activation=\"relu\"),\n",
        "      layers.MaxPooling1D(),\n",
        "      layers.Conv1D(496, 1, activation=\"relu\"),\n",
        "      layers.MaxPooling1D(), \n",
        "      layers.Dropout(0.1),\n",
        "      layers.Conv1D(320, 1, activation=\"relu\"),\n",
        "      layers.MaxPooling1D(),\n",
        "\n",
        "      layers.LSTM(128, activation='tanh',return_sequences=True,dropout=0.1),\n",
        "      #layers.BatchNormalization(),\n",
        "      #layers.LayerNormalization(),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(units=64,activation='relu'),\n",
        "      layers.Dropout(0.1),\n",
        "      layers.Dense(units=64,activation='relu'),\n",
        "\n",
        "      layers.Dense(3, activation= 'softmax')\n",
        "])\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78oX18V1IGRE"
      },
      "source": [
        "> Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5MnuzokIGYL"
      },
      "source": [
        "model.compile(optimizer='Adamax',#'Adam','rmsprop'\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])#MeanSquaredError\n",
        "              \n",
        "model.fit(X, Y, batch_size=64, epochs=10, validation_data=(x_val, y_val), callbacks=my_callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBUM9b8YIbmu"
      },
      "source": [
        "> Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzlALZDwvTuM"
      },
      "source": [
        "name_best_model =\"best_model_1.keras\"\n",
        "model.save(name_best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ42lAkkwx0a"
      },
      "source": [
        "#COPY TO DRIVE\n",
        "#!cp name_best_model \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCgwqtlJupAf"
      },
      "source": [
        "## Predict data and Visualization of results\n",
        "\n",
        "\n",
        "> https://stats.stackexchange.com/questions/276067/whats-considered-a-good-log-loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDOIXrRETAxx"
      },
      "source": [
        "#Load bestmodel for Hybrid model \n",
        "#model.load_weights(\"beast_model_m.keras\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVB-2n83ozpN"
      },
      "source": [
        "#vetorizer de dataset test\n",
        "x_test = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "#LOAD BEST EPOCH\n",
        "#model.load_weights(filename)#load best epoch\n",
        "#model.evaluate(x_test,y_test) \n",
        "\n",
        "#model predict\n",
        "preds=np.round(model.predict(x_test),0)\n",
        "pred_labels = np.argmax(preds, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmeMV94lXBJz"
      },
      "source": [
        "#create confusion_matrix on heatmap \n",
        "cm = confusion_matrix(np.array(test_labels),np.array(pred_labels))\n",
        "labels = ['Negative', 'Neutro','Positive'] # labels for y-axis\n",
        "labels2 = ['P_Negative', 'P_Neutro','P_Positive'] # labels for x-axis\n",
        "sns.heatmap(#cm, fmt='g',\n",
        "            cm/np.sum(cm), fmt='.2%', \n",
        "            annot=True,\n",
        "            cmap='Blues',xticklabels=labels2, yticklabels=labels)\n",
        "\n",
        "#report for metrics\n",
        "cm=metrics.classification_report(np.array(test_labels),np.array(pred_labels),target_names=[\"Negative\",\"Neutro\",\"Positive\"])\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}